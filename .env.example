# Configuração do Google Gemini API
# Exemplo de arquivo .env para configuração local
# Copie este arquivo para .env e preencha com seus valores

# Para obter sua chave API:
# 1. Acesse: https://aistudio.google.com/app/apikey
# 2. Clique em "Create API Key"
# 3. Copie a chave e cole abaixo

# Google Cloud Platform
GCP_PROJECT_ID=seu-project-id-aqui
GCP_LOCATION=us-central1

GEMINI_API_KEY=sua-chave-aqui

# Opcional: Se usar Service Account Key
# GOOGLE_APPLICATION_CREDENTIALS=/path/to/key.json

# Opcional: Modelo a ser usado (padrão: gemini-2.5-flash)
# Opções: gemini-2.5-flash, gemini-2.5-pro, gemini-pro
GEMINI_MODEL=gemini-2.5-flash

# ============================================================================
# RATE LIMITING - Configurações para evitar atingir limites da API
# ============================================================================
# Referência: https://ai.google.dev/gemini-api/docs/rate-limits
#
# Limites do Free Tier:
# - Gemini 2.5 Flash: 10 RPM (Requests Per Minute), 250k TPM (Tokens Per Minute)
# - Gemini 2.5 Pro: 2 RPM, 125k TPM
#
# Limites do Nível 1 (com faturamento):
# - Gemini 2.5 Flash: 1000 RPM, 4M TPM
# - Gemini 2.5 Pro: 360 RPM, 4M TPM

# Delay entre requisições (em milissegundos)
# Free Tier recomendado: 6000ms (6s) = máximo 10 requests/minuto com margem de segurança
# Nível 1 pode usar: 100ms (muito mais rápido)
REQUEST_DELAY_MS=6000

# Intervalo de checkpoint - salva progresso a cada N questões
# Útil para retomar processamento em caso de erro ou interrupção
CHECKPOINT_INTERVAL=5

# ============================================================================
# CONFIGURAÇÕES DE RAG - Chunking e Retrieval
# ============================================================================

# Tamanho máximo de cada chunk (segmento) em tokens
# Chunks menores = mais precisão, mas mais processamento
# Chunks maiores = mais contexto, mas menos precisão
# Recomendado para FRs: 1000-1500 tokens (captura tabelas completas)
# Padrão: 1200 tokens (~900 palavras, ~6 parágrafos)
MAX_SEGMENT_SIZE_IN_TOKENS=1200

# Overlap entre chunks consecutivos (em tokens)
# Ajuda a não perder contexto nas divisões
# Recomendado: 10-20% do tamanho do chunk
# Padrão: 200 tokens (~16% de 1200)
SEGMENT_OVERLAP_IN_TOKENS=200

# Quantidade máxima de chunks recuperados na busca
# Mais resultados = mais contexto, mas prompt maior e mais caro
# Recomendado: 10-20 chunks
# Padrão: 15 chunks
MAX_RESULTS_FOR_RETRIEVAL=15

# Score mínimo de similaridade (0.0 a 1.0)
# Score muito alto = pode não encontrar nada
# Score muito baixo = pode trazer contexto irrelevante
# Recomendado para tabelas: 0.55-0.65
# Padrão: 0.60
MIN_SCORE_FOR_RETRIEVAL=0.60
