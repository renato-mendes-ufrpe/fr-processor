package com.example.rag.indexer;

import com.example.rag.config.Config;
import dev.langchain4j.data.document.Document;
import dev.langchain4j.data.document.DocumentSplitter;
import dev.langchain4j.data.document.parser.apache.tika.ApacheTikaDocumentParser;
import dev.langchain4j.data.document.splitter.DocumentSplitters;
import dev.langchain4j.data.segment.TextSegment;
import dev.langchain4j.model.embedding.EmbeddingModel;
import dev.langchain4j.model.embedding.onnx.allminilml6v2.AllMiniLmL6V2EmbeddingModel;
import dev.langchain4j.store.embedding.EmbeddingStore;
import dev.langchain4j.store.embedding.EmbeddingStoreIngestor;
import dev.langchain4j.store.embedding.inmemory.InMemoryEmbeddingStore;

import java.io.FileInputStream;
import java.io.InputStream;
import java.nio.file.Path;
import java.nio.file.Paths;

/**
 * Respons√°vel pela indexa√ß√£o de documentos PDF no sistema RAG.
 * 
 * O QUE √â INDEXA√á√ÉO:
 * Indexa√ß√£o √© o processo de transformar documentos em uma representa√ß√£o vetorial
 * que permite buscas por similaridade sem√¢ntica (por significado, n√£o apenas palavras).
 * 
 * PIPELINE DE INDEXA√á√ÉO (5 ETAPAS):
 * 
 * 1. CARREGAMENTO
 *    - L√™ o arquivo PDF do disco
 *    - Converte para objeto Document do LangChain4j
 * 
 * 2. PARSING (An√°lise)
 *    - Extrai o texto do PDF usando Apache Tika
 *    - Remove formata√ß√£o, mant√©m apenas conte√∫do textual
 * 
 * 3. CHUNKING (Divis√£o)
 *    - Divide o texto em peda√ßos menores (chunks/segments)
 *    - Usa DocumentSplitter recursivo com overlap
 *    - Cada chunk tem ~2000 tokens com 600 tokens de sobreposi√ß√£o
 * 
 * 4. EMBEDDING (Vetoriza√ß√£o)
 *    - Converte cada chunk em um vetor num√©rico (embedding)
 *    - Usa modelo AllMiniLmL6V2 (local, offline, 384 dimens√µes)
 *    - Embeddings capturam o significado sem√¢ntico do texto
 * 
 * 5. ARMAZENAMENTO
 *    - Salva os embeddings no EmbeddingStore (banco de vetores em mem√≥ria)
 *    - Permite buscas posteriores por similaridade
 * 
 * MODELO DE EMBEDDINGS:
 * - Nome: AllMiniLmL6V2
 * - Tipo: ONNX (Open Neural Network Exchange)
 * - Tamanho: ~80 MB
 * - Dimens√µes: 384
 * - Caracter√≠sticas: R√°pido, leve, roda localmente sem API
 * - Qualidade: Excelente para busca sem√¢ntica em portugu√™s
 * 
 * USO:
 * DocumentIndexer indexer = new DocumentIndexer();
 * indexer.indexDocument(caminhoArquivo);
 * EmbeddingStore store = indexer.getEmbeddingStore();
 */
public class DocumentIndexer {
    
    /**
     * Armazena os embeddings (vetores) dos chunks de texto na mem√≥ria.
     * 
     * O InMemoryEmbeddingStore √© um banco de dados vetorial simples que:
     * - Armazena pares de (TextSegment, Embedding)
     * - Permite busca por similaridade usando dist√¢ncia cosseno
     * - Roda em mem√≥ria RAM (r√°pido, mas perde dados ao fechar)
     * 
     * Para produ√ß√£o, considere usar stores persistentes como:
     * - Pinecone
     * - Weaviate
     * - Chroma
     * - Milvus
     */
    private final InMemoryEmbeddingStore<TextSegment> embeddingStore;
    
    /**
     * Modelo que converte texto em embeddings (vetores num√©ricos).
     * 
     * AllMiniLmL6V2 √© um modelo de embeddings:
     * - Baseado em Sentence Transformers
     * - Otimizado para busca sem√¢ntica
     * - Roda localmente (n√£o precisa de internet ou API key)
     * - Download autom√°tico na primeira execu√ß√£o (~80 MB)
     * 
     * O modelo gera vetores de 384 dimens√µes que capturam
     * o significado sem√¢ntico do texto de entrada.
     */
    private final EmbeddingModel embeddingModel;
    
    /**
     * Construtor da classe DocumentIndexer.
     * 
     * Inicializa os componentes necess√°rios:
     * 1. InMemoryEmbeddingStore - Banco de vetores em mem√≥ria
     * 2. AllMiniLmL6V2EmbeddingModel - Modelo de embeddings local
     * 
     * Nota: O modelo √© baixado automaticamente na primeira execu√ß√£o
     * e fica em cache para usos futuros (~80 MB).
     */
    public DocumentIndexer() {
        this.embeddingStore = new InMemoryEmbeddingStore<>();
        this.embeddingModel = new AllMiniLmL6V2EmbeddingModel();
        
        System.out.println("‚úÖ DocumentIndexer inicializado");
        System.out.println("   Embedding Model: AllMiniLmL6V2 (384 dimens√µes, local)");
    }
    
    /**
     * Indexa um documento PDF completo no sistema RAG.
     * 
     * PROCESSO COMPLETO:
     * 
     * 1. Valida se o arquivo existe
     * 2. Carrega o PDF usando Apache Tika
     * 3. Divide o documento em chunks usando DocumentSplitter recursivo
     * 4. Gera embeddings para cada chunk usando AllMiniLmL6V2
     * 5. Armazena os embeddings no InMemoryEmbeddingStore
     * 
     * PAR√ÇMETROS DE CHUNKING (definidos em Config.java):
     * - Tamanho m√°ximo: 500 tokens (~375 palavras)
     * - Overlap: 50 tokens (~37 palavras)
     * - Estrat√©gia: Recursiva (tenta manter par√°grafos inteiros)
     * 
     * PERFORMANCE:
     * - PDF pequeno (100 p√°ginas): ~30 segundos
     * - PDF m√©dio (500 p√°ginas): ~2 minutos
     * - PDF grande (1000 p√°ginas): ~5 minutos
     * 
     * @param pdfFilePath Caminho completo para o arquivo PDF a ser indexado
     *                    Exemplo: "data/report/documento.pdf"
     * @throws RuntimeException Se o arquivo n√£o existir ou houver erro no processamento
     */
    public void indexDocument(String pdfFilePath) {
        try {
            System.out.println("üìÑ Iniciando indexa√ß√£o do documento: " + pdfFilePath);
            
            // ETAPA 1: Carregar o arquivo PDF
            Path path = Paths.get(pdfFilePath);
            System.out.println("   [1/5] Carregando arquivo PDF...");
            
            // ETAPA 2: Parse do PDF com Apache Tika
            // Apache Tika √© uma biblioteca universal de parsing que suporta:
            // - PDF, DOCX, PPTX, XLSX
            // - HTML, XML, TXT
            // - Imagens com OCR (se configurado)
            ApacheTikaDocumentParser parser = new ApacheTikaDocumentParser();
            Document document;
            try (InputStream inputStream = new FileInputStream(path.toFile())) {
                document = parser.parse(inputStream);
            }
            System.out.println("   [2/5] Parsing conclu√≠do: " + document.text().length() + " caracteres");
            
            // ETAPA 3: Dividir documento em chunks
            // DocumentSplitter recursivo tenta manter a estrutura do texto:
            // - Primeiro tenta dividir por par√°grafos duplos (\n\n)
            // - Se o chunk for muito grande, divide por par√°grafos simples (\n)
            // - Se ainda for grande, divide por senten√ßas (.)
            // - Como √∫ltimo recurso, divide por palavras
            System.out.println("   [3/5] Dividindo em chunks...");
            DocumentSplitter splitter = DocumentSplitters.recursive(
                Config.MAX_SEGMENT_SIZE_IN_TOKENS,
                Config.SEGMENT_OVERLAP_IN_TOKENS
            );

            // ETAPA 4: Criar o Ingestor (processador de ingest√£o)
            // EmbeddingStoreIngestor coordena o processo de:
            // 1. Pegar cada chunk do DocumentSplitter
            // 2. Enviar para o EmbeddingModel gerar o embedding
            // 3. Armazenar o par (chunk, embedding) no EmbeddingStore
            System.out.println("   [4/5] Gerando embeddings (pode demorar alguns minutos)...");
            EmbeddingStoreIngestor ingestor = EmbeddingStoreIngestor.builder()
                .documentSplitter(splitter)
                .embeddingModel(embeddingModel)
                .embeddingStore(embeddingStore)
                .build();
            
            // ETAPA 5: Executar a ingest√£o completa
            // Este m√©todo bloqueia at√© processar todo o documento
            ingestor.ingest(document);
            
            // Exibe estat√≠sticas finais
            System.out.println("   [5/5] Indexa√ß√£o conclu√≠da!");
            System.out.println("   ‚úÖ Documento indexado com sucesso");
            System.out.println("   ‚úÖ Embeddings armazenados em mem√≥ria");
            
        } catch (Exception e) {
            System.err.println("‚ùå Erro ao indexar documento: " + e.getMessage());
            e.printStackTrace();
            throw new RuntimeException("Falha na indexa√ß√£o", e);
        }
    }
    
    /**
     * Retorna o EmbeddingStore contendo todos os embeddings indexados.
     * 
     * Este m√©todo √© usado por RagQueryEngine para realizar buscas
     * por similaridade no conjunto de chunks indexados.
     * 
     * COMO USAR:
     * EmbeddingStore store = indexer.getEmbeddingStore();
     * // Passar store para RagQueryEngine
     * 
     * ESTRUTURA DO STORE:
     * O InMemoryEmbeddingStore cont√©m:
     * - Lista de TextSegments (chunks de texto)
     * - Lista de Embeddings (vetores de 384 dimens√µes)
     * - √çndice interno para busca eficiente por similaridade
     * 
     * @return EmbeddingStore contendo todos os chunks e seus embeddings
     */
    public EmbeddingStore<TextSegment> getEmbeddingStore() {
        return embeddingStore;
    }
    
    /**
     * Retorna o modelo de embeddings utilizado na indexa√ß√£o.
     * 
     * O mesmo modelo deve ser usado tanto na indexa√ß√£o quanto
     * na busca (query time) para garantir consist√™ncia.
     * 
     * POR QUE ISSO √â IMPORTANTE:
     * - Embeddings de modelos diferentes n√£o s√£o compar√°veis
     * - Cada modelo tem seu pr√≥prio "espa√ßo vetorial"
     * - Usar modelos diferentes = busca n√£o funciona corretamente
     * 
     * COMO USAR:
     * EmbeddingModel model = indexer.getEmbeddingModel();
     * // Passar model para RagQueryEngine
     * 
     * @return Inst√¢ncia do AllMiniLmL6V2EmbeddingModel usado na indexa√ß√£o
     */
    public EmbeddingModel getEmbeddingModel() {
        return embeddingModel;
    }
}
