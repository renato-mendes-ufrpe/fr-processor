package com.example.rag.retrieval;

import com.example.rag.config.Config;
import dev.langchain4j.data.embedding.Embedding;
import dev.langchain4j.data.segment.TextSegment;
import dev.langchain4j.model.chat.ChatModel;
import dev.langchain4j.model.embedding.EmbeddingModel;
import dev.langchain4j.model.googleai.GoogleAiGeminiChatModel;
import dev.langchain4j.store.embedding.EmbeddingMatch;
import dev.langchain4j.store.embedding.EmbeddingSearchRequest;
import dev.langchain4j.store.embedding.EmbeddingSearchResult;
import dev.langchain4j.store.embedding.EmbeddingStore;
import java.time.Duration;
import java.util.List;
import java.util.stream.Collectors;

/**
 * Motor de consulta RAG (Retrieval-Augmented Generation).
 * 
 * Respons√°vel pelas fases de RETRIEVAL e GENERATION do pipeline RAG.
 * 
 * O QUE ESTE MOTOR FAZ:
 * 
 * ETAPA 1: RETRIEVAL (Recupera√ß√£o)
 * - Recebe uma pergunta do usu√°rio em linguagem natural
 * - Converte a pergunta em embedding (vetor de 384 dimens√µes)
 * - Busca por similaridade cosseno no EmbeddingStore
 * - Retorna os chunks mais relevantes com seus scores de similaridade
 * 
 * ETAPA 2: GENERATION (Gera√ß√£o - Opcional)
 * - Constr√≥i um contexto aumentado com os chunks encontrados
 * - Cria um prompt especial incluindo contexto + pergunta
 * - Envia para o Google Gemini (se configurado)
 * - Retorna a resposta gerada pelo modelo
 * 
 * BUSCA POR SIMILARIDADE:
 * Usa dist√¢ncia cosseno para encontrar os embeddings mais pr√≥ximos.
 * 
 * Exemplo de scores:
 * - 1.0 = Id√™ntico (imposs√≠vel para textos diferentes)
 * - 0.9-1.0 = Extremamente similar
 * - 0.8-0.9 = Muito similar (geralmente relevante)
 * - 0.7-0.8 = Similar (pode ser relevante)
 * - Abaixo de 0.7 = Menos similar (configur√°vel)
 * 
 * MODOS DE OPERA√á√ÉO:
 * 
 * 1. Modo Completo (com Gemini configurado):
 *    query("pergunta") ‚Üí retorna resposta gerada pelo Gemini
 * 
 * 2. Modo Somente Retrieval (sem Gemini):
 *    query("pergunta") ‚Üí retorna contexto recuperado (sem gera√ß√£o)
 *    retrieveOnly("pergunta") ‚Üí retorna lista de matches
 * 
 * CONFIGURA√á√ÉO DO GEMINI:
 * - Model: gemini-2.5-flash (configur√°vel via .env)
 * - Temperature: 0.7 (equil√≠brio entre criatividade e consist√™ncia)
 * - Max Retries: 3 tentativas em caso de erro
 * - Timeout: 30 segundos por chamada
 * 
 * USO B√ÅSICO:
 * EmbeddingStore store = indexer.getEmbeddingStore();
 * EmbeddingModel model = indexer.getEmbeddingModel();
 * RagQueryEngine engine = new RagQueryEngine(store, model);
 * String resposta = engine.query("Qual o principal neg√≥cio da empresa?");
 */
public class RagQueryEngine {
    
    /**
     * Armazena os embeddings (vetores) indexados na fase de indexa√ß√£o.
     * 
     * O EmbeddingStore funciona como um banco de dados vetorial que permite
     * buscar chunks de texto por similaridade sem√¢ntica.
     * 
     * Este √© o mesmo store criado e populado pelo DocumentIndexer.
     */
    private final EmbeddingStore<TextSegment> embeddingStore;
    
    /**
     * Modelo que converte texto em embeddings (vetores).
     * 
     * IMPORTANTE: Deve ser o MESMO modelo usado na indexa√ß√£o!
     * 
     * Se usar modelos diferentes:
     * - Na indexa√ß√£o: AllMiniLmL6V2
     * - Na query: Outro modelo
     * = Busca n√£o funcionar√° (vetores incompar√°veis)
     * 
     * Modelo atual: AllMiniLmL6V2 (local, 384 dimens√µes)
     */
    private final EmbeddingModel embeddingModel;
    
    /**
     * Modelo de linguagem (LLM) usado para gerar respostas.
     * 
     * Configurado apenas se GEMINI_API_KEY estiver presente no .env
     * 
     * Se null:
     * - Sistema opera em modo "somente retrieval"
     * - Retorna apenas o contexto recuperado, sem gera√ß√£o
     * 
     * Se configurado:
     * - GoogleAiGeminiChatModel com gemini-2.5-flash
     * - Gera respostas baseadas no contexto recuperado
     */
    private final ChatModel chatModel;
    
    /**
     * Construtor do motor de consulta RAG.
     * 
     * Inicializa o motor com os componentes necess√°rios:
     * 1. EmbeddingStore - Banco de vetores com chunks indexados
     * 2. EmbeddingModel - Modelo para converter queries em embeddings
     * 3. ChatModel - (Opcional) Gemini para gera√ß√£o de respostas
     * 
     * O Gemini √© inicializado automaticamente se:
     * - Arquivo .env existe
     * - GEMINI_API_KEY est√° configurada
     * - Chave n√£o est√° vazia ou placeholder
     * 
     * PAR√ÇMETROS DO GEMINI:
     * - apiKey: Lida do .env
     * - modelName: gemini-2.5-flash (configur√°vel)
     * - temperature: 0.7 (criatividade moderada)
     * - maxRetries: 3 (tentativas em caso de erro)
     * - timeout: 30 segundos
     * 
     * @param embeddingStore Store contendo todos os chunks indexados
     * @param embeddingModel Modelo de embeddings (deve ser o mesmo da indexa√ß√£o)
     */
    public RagQueryEngine(EmbeddingStore<TextSegment> embeddingStore, 
                          EmbeddingModel embeddingModel) {
        this.embeddingStore = embeddingStore;
        this.embeddingModel = embeddingModel;
        
        // Inicializa Gemini se estiver configurado
        if (Config.isGeminiConfigured()) {
            this.chatModel = GoogleAiGeminiChatModel.builder()
                    .apiKey(Config.GEMINI_API_KEY)
                    .modelName(Config.GEMINI_MODEL)
                    .temperature(0.0) // Determin√≠stico
                    .maxRetries(3)
                    .timeout(Duration.ofSeconds(30))
                    .build();
            System.out.println("‚úÖ RagQueryEngine inicializado com Gemini (" + Config.GEMINI_MODEL + ", temperature=0.0)");
        } else {
            this.chatModel = null;
            System.out.println("‚úÖ RagQueryEngine inicializado (somente retrieval - sem Gemini)");
        }
    }
    
    /**
     * Executa uma consulta RAG completa: recupera√ß√£o + gera√ß√£o (opcional).
     * 
     * FLUXO DE EXECU√á√ÉO:
     * 
     * 1. VETORIZA√á√ÉO DA QUERY
     *    - Converte a pergunta do usu√°rio em embedding
     *    - Usa o mesmo modelo da indexa√ß√£o (AllMiniLmL6V2)
     * 
     * 2. BUSCA POR SIMILARIDADE
     *    - Compara o embedding da query com todos os chunks indexados
     *    - Usa dist√¢ncia cosseno (quanto mais pr√≥ximo de 1.0, mais similar)
     *    - Filtra por score m√≠nimo (0.7 por padr√£o)
     *    - Retorna at√© 5 chunks mais relevantes
     * 
     * 3. CONSTRU√á√ÉO DO CONTEXTO
     *    - Concatena os chunks recuperados
     *    - Separa com marcadores "---"
     * 
     * 4. CRIA√á√ÉO DO PROMPT AUMENTADO
     *    - Instru√ß√£o para o LLM: "Use apenas estas informa√ß√µes"
     *    - Contexto: Chunks recuperados
     *    - Pergunta: Query do usu√°rio
     * 
     * 5. GERA√á√ÉO (SE GEMINI CONFIGURADO)
     *    - Envia prompt para o Gemini
     *    - Retorna resposta gerada
     *    - Em caso de erro, retorna o prompt sem resposta
     * 
     * 6. RETORNO (SE GEMINI N√ÉO CONFIGURADO)
     *    - Retorna apenas o prompt aumentado
     *    - √ötil para testar retrieval ou usar outro LLM
     * 
     * EXEMPLO DE USO:
     * String resposta = engine.query("Qual o principal neg√≥cio da empresa?");
     * System.out.println(resposta);
     * 
     * EXEMPLO DE SA√çDA (com Gemini):
     * "A Ambipar √© especializada em gest√£o de res√≠duos e emerg√™ncias ambientais..."
     * 
     * EXEMPLO DE SA√çDA (sem Gemini):
     * "Voc√™ √© um assistente...
     *  DOCUMENTOS:
     *  [contexto recuperado]
     *  PERGUNTA: Qual o principal neg√≥cio da empresa?"
     * 
     * @param userQuestion Pergunta do usu√°rio em linguagem natural
     * @return Resposta gerada (com Gemini) ou prompt aumentado (sem Gemini)
     */
    public String query(String userQuestion) {
        System.out.println("\nüîç Processando query: \"" + userQuestion + "\"");
        
        // 1. Converter a pergunta em embedding
        System.out.println("   üîÑ Gerando embedding da query...");
        Embedding queryEmbedding = embeddingModel.embed(userQuestion).content();
        
        // 2. Buscar documentos similares
        System.out.println("   üîé Buscando documentos relevantes...");
        EmbeddingSearchRequest searchRequest = EmbeddingSearchRequest.builder()
                .queryEmbedding(queryEmbedding)
                .maxResults(Config.MAX_RESULTS_FOR_RETRIEVAL)
                .minScore(Config.MIN_SCORE_FOR_RETRIEVAL)
                .build();
        
        EmbeddingSearchResult<TextSegment> searchResult = embeddingStore.search(searchRequest);
        List<EmbeddingMatch<TextSegment>> matches = searchResult.matches();
        
        System.out.println("   ‚úÖ Encontrados " + matches.size() + " documentos relevantes");
        
        // 3. Imprimir os matches
        printMatches(matches);
        
        // 4. Construir contexto aumentado
        String context = buildContext(matches);
        
        // 5. Criar prompt aumentado
        String augmentedPrompt = buildAugmentedPrompt(userQuestion, context);
        
        System.out.println("\n   üí° Contexto recuperado com sucesso!");
        System.out.println("   üìä Total de caracteres no contexto: " + context.length());
        
        // 6. Se Gemini estiver configurado, gerar resposta
        if (chatModel != null) {
            System.out.println("   ü§ñ Enviando para Gemini...");
            try {
                String answer = chatModel.chat(augmentedPrompt);
                System.out.println("   ‚úÖ Resposta recebida do Gemini");
                return answer;
            } catch (Exception e) {
                // Log detalhado da exce√ß√£o, mas N√ÉO retornar o prompt como resposta.
                // Retornar null permite que o QuestionProcessor trate como "INFORMA√á√ÉO N√ÉO ENCONTRADA".
                System.err.println("   ‚ùå Erro ao chamar Gemini: " + e.getClass().getSimpleName() + " - " + e.getMessage());
                e.printStackTrace(System.err);
                return null;
            }
        } else {
            // Sem Gemini, retorna apenas o prompt aumentado
            return augmentedPrompt;
        }
    }
    
    /**
     * Executa busca somente por retrieval, sem gera√ß√£o de resposta.
     * 
     * Este m√©todo √© √∫til para:
     * - Testar a qualidade da recupera√ß√£o de documentos
     * - Debug: ver quais chunks est√£o sendo encontrados
     * - Usar o contexto recuperado em outro LLM
     * - Implementar l√≥gica customizada de gera√ß√£o
     * 
     * DIFEREN√áA DO M√âTODO query():
     * - query(): Retrieval + Gera√ß√£o (com Gemini, se configurado)
     * - retrieveOnly(): Apenas retrieval (retorna matches brutos)
     * 
     * RETORNO:
     * Lista de EmbeddingMatch contendo:
     * - embedded: TextSegment com o texto do chunk
     * - score: Double com similaridade (0.0 a 1.0)
     * - embeddingId: ID √∫nico do embedding
     * 
     * EXEMPLO DE USO:
     * List matches = engine.retrieveOnly("Qual o principal neg√≥cio?");
     * for (EmbeddingMatch match : matches) {
     *     System.out.println("Score: " + match.score());
     *     System.out.println("Texto: " + match.embedded().text());
     * }
     * 
     * @param userQuestion Pergunta do usu√°rio em linguagem natural
     * @return Lista de matches ordenados por similaridade (maior para menor)
     */
    public List<EmbeddingMatch<TextSegment>> retrieveOnly(String userQuestion) {
        System.out.println("\nüîç Modo Retrieval Only: \"" + userQuestion + "\"");
        
        Embedding queryEmbedding = embeddingModel.embed(userQuestion).content();
        
        EmbeddingSearchRequest searchRequest = EmbeddingSearchRequest.builder()
                .queryEmbedding(queryEmbedding)
                .maxResults(Config.MAX_RESULTS_FOR_RETRIEVAL)
                .minScore(Config.MIN_SCORE_FOR_RETRIEVAL)
                .build();
        
        EmbeddingSearchResult<TextSegment> searchResult = embeddingStore.search(searchRequest);
        List<EmbeddingMatch<TextSegment>> matches = searchResult.matches();
        
        System.out.println("   ‚úÖ Encontrados " + matches.size() + " documentos");
        printMatches(matches);
        
        return matches;
    }
    
    /**
     * Constr√≥i o contexto aumentado a partir dos chunks recuperados.
     * 
     * Pega todos os TextSegments dos matches e concatena em uma string √∫nica.
     * Cada chunk √© separado por "---" para facilitar a leitura.
     * 
     * Se n√£o houver matches (nenhum documento relevante encontrado),
     * retorna uma mensagem informando isso.
     * 
     * FORMATO DO CONTEXTO:
     * 
     * [Texto do chunk 1]
     * 
     * ---
     * 
     * [Texto do chunk 2]
     * 
     * ---
     * 
     * [Texto do chunk 3]
     * 
     * @param matches Lista de matches retornados pela busca de similaridade
     * @return String contendo o contexto concatenado ou mensagem de "nenhum documento encontrado"
     */
    private String buildContext(List<EmbeddingMatch<TextSegment>> matches) {
        if (matches.isEmpty()) {
            return "Nenhum documento relevante foi encontrado.";
        }
        
        return matches.stream()
                .map(match -> match.embedded().text())
                .collect(Collectors.joining("\n\n---\n\n"));
    }
    
    /**
     * Constr√≥i o prompt aumentado para envio ao LLM.
     * 
     * Este prompt segue o padr√£o RAG:
     * 1. Instru√ß√µes ao modelo (papel, comportamento esperado)
     * 2. Contexto recuperado (chunks relevantes)
     * 3. Pergunta do usu√°rio
     * 4. Marcador de resposta
     * 
     * INSTRU√á√ïES AO MODELO:
     * - Voc√™ √© um assistente especializado
     * - Use APENAS as informa√ß√µes fornecidas nos documentos
     * - Se n√£o souber, admita ("n√£o tenho informa√ß√£o suficiente")
     * - N√£o invente informa√ß√µes (previne alucina√ß√µes)
     * 
     * POR QUE ISSO PREVINE ALUCINA√á√ïES:
     * - O LLM √© explicitamente instru√≠do a usar apenas o contexto
     * - Se o contexto n√£o cont√©m a resposta, o modelo deve admitir
     * - Reduz significativamente respostas inventadas
     * 
     * FORMATO DO PROMPT:
     * ```
     * Voc√™ √© um assistente especializado em an√°lise de relat√≥rios...
     * 
     * Use as seguintes informa√ß√µes dos documentos para responder...
     * Se a resposta n√£o puder ser encontrada nos documentos, diga...
     * 
     * DOCUMENTOS:
     * [contexto]
     * 
     * PERGUNTA DO USU√ÅRIO:
     * [pergunta]
     * 
     * RESPOSTA:
     * ```
     * 
     * Este prompt pode ser enviado para qualquer LLM (GPT, Claude, Gemini, Llama, etc)
     * 
     * @param userQuestion Pergunta original do usu√°rio
     * @param context Contexto recuperado (chunks concatenados)
     * @return Prompt completo formatado para o LLM
     */
    private String buildAugmentedPrompt(String userQuestion, String context) {
        return String.format("""
                Voc√™ √© um assistente especializado em an√°lise de relat√≥rios financeiros e empresariais.
                
                Use as seguintes informa√ß√µes dos documentos para responder a pergunta do usu√°rio.
                Se a resposta n√£o puder ser encontrada nos documentos fornecidos, diga que n√£o tem 
                informa√ß√£o suficiente para responder.
                
                DOCUMENTOS:
                %s
                
                PERGUNTA DO USU√ÅRIO:
                %s
                
                RESPOSTA:
                """, context, userQuestion);
    }
    
    /**
     * Exibe no console os chunks recuperados com seus scores de similaridade.
     * 
     * Para cada match, mostra:
     * - N√∫mero do resultado (1, 2, 3...)
     * - Score de similaridade (0.0 a 1.0)
     * - Preview do texto (primeiros 150 caracteres)
     * 
     * INTERPRETA√á√ÉO DOS SCORES:
     * - 0.90-1.00: Extremamente relevante
     * - 0.80-0.90: Muito relevante
     * - 0.70-0.80: Relevante (threshold padr√£o: 0.7)
     * - Abaixo de 0.70: Pouco relevante (filtrado)
     * 
     * FORMATO DA SA√çDA:
     * 
     *    üìÑ Documentos recuperados:
     *       [1] Score: 0.8466 | Preview: Os principais mecanismos...
     *       [2] Score: 0.8360 | Preview: Principais insumos e...
     *       [3] Score: 0.8356 | Preview: Em rela√ß√£o ao √∫ltimo...
     * 
     * Os newlines (\n) no texto s√£o substitu√≠dos por espa√ßos para melhor visualiza√ß√£o.
     * 
     * @param matches Lista de matches com chunks e scores
     */
    private void printMatches(List<EmbeddingMatch<TextSegment>> matches) {
        System.out.println("\n   üìÑ Documentos recuperados:");
        for (int i = 0; i < matches.size(); i++) {
            EmbeddingMatch<TextSegment> match = matches.get(i);
            String preview = match.embedded().text().substring(0, 
                    Math.min(150, match.embedded().text().length()));
            System.out.printf("      [%d] Score: %.4f | Preview: %s...%n", 
                    i + 1, match.score(), preview.replace("\n", " "));
        }
        System.out.println();
    }
}
